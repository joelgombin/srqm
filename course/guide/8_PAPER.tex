\chapter{Research projects}%
	\label{ch:paper}

\newthought{The research project} is the graded coursework component that you will work on with a student partner throughout the entire semester. It consists in regularly submitting a draft paper with its replication script, in the form of a Stata do-file.

In the first half of the course, you will submit and revise the \emph{first draft} of your project. The exact deadline for submission will be discussed in class. More important is the overall planning that you will have to follow until then:

\begin{enumerate}
  \item In Week~1, explore the course datasets.
  \item In Week~2, register yourself in the class projects list.
  \item In Week~3, register your \emph{group} in the class projects list.
  \item In Week~4, download the first draft template.
  \item In Week~5, submit your first draft.
  \item In Week~6, correct your first draft.
\end{enumerate}

In the second part of the course, you will resubmit a \emph{revised draft}, and then finally hand in your \emph{final paper} by the end of the course. The final deadline is generally set to ``Week~13'', one week after the last course session.

\section{Principles}

Keep it simple. Choose a simple opening line that states the most salient characteristic of the issue under scrutiny. ``Cancer commissioning is complex'' or ``Survey weighting is a mess'' are both real, and acceptable, examples.

\subsection{Parsimony}

Substantively, your model is about explaining some relationships between social forces. Statistically, it is about understanding variance by predicting it. for your paper, select only what is needed from your statistical analysis to test your substantive theory. Leave the `tech specs' in your do-file, with comments in the source, and aim at writing an argument that will be based on your expertise rather than on computer code.

Be parsimonious in your approach to modeling: select your findings to the bare logical minimum required by your analysis. Your do-file will contain tons of tests and models, but do not publish every intermediate result that was taken in your analysis: report statistically significant findings for which you can provide an interpretation, and document negative results when they are surprising with reference to your hypotheses.

\paragraph{Tables and numbers} Even in a quantitative environment, you should feel some pressure to reduce the number of numbers in your work to their bare minimum. There are only a few requirements, such as summary statistics or regression estimates. In many circumstances, you might want to leave any other result in your do-file. This, for instance, applies to several crosstabulations that you might have run until now, although you can cite particular percentages or tests.

A simple example of parsimony in numbers concerns precision. Many social science measurements have no serious degree of precision: measures of social attitudes, for example, are taken on 11-point scales at most. If you report value with a lot of digits after the decimal separator, as in \texttt{8.4759}, you are suggesting that your data are reliable at that level of precision, which is very rarely true. Round up, therefore, all numbers to 0, 1 or 2 decimals.

\paragraph{Figures} It is an excellent idea to visually inspect the distributions and relationships in your data. It is also an excellent idea to use graphs where tables could have done but would have carried less information to the reader. Finally, it is great to use graph options to make Stata graphs look a tad better. The gods of data visualization, and their strength is growing in this early century, will be pleased by all that.\footcite{Tufte:2001t}

Your do-file contains a lot of graphic output, and you will have to select figures based on their informative content.

\paragraph{Significance tests} Remember that the principle of statistical significance is to estimate the probability level of the null hypothesis, i.e. an abstract situation where, to make it simple, the variations in your data are compared to the absence of any significant variation. `Rejecting the null' is actually often trivial: you will always find some covariance in social data, even if it is completely spurious and/or statistically powerless.

For these reasons, report $p$-values in your text only when you feel that the reader might want to see it: if you are making an important claim, or if you are interpreting a coefficient or test with a high $p$-value but which you still believe to be significant (recall what was mentioned several times about the freedom of judgement that you should exert around the conventional boundary of $p < .05$).

\subsection{Validation}

Your research question and your statistical model are tied together but you can very well get an answer from one without getting an answer from the other. In other terms, you might be able to secure insights from negative results, which is good news. On the other hand, your model can also make a `Type III error' and bring the right answer to the wrong question.\footnote{Originally from Peter Kennedy's ``The Ten Commandments of Applied Econometrics,'' cited by Christopher S.~McIntosh at \url{http://courses.cals.uidaho.edu/aers/agecon525/sp2010/Lecture21DoingAppliedEconometrics.pdf}.}

\paragraph{Negative findings} Negative findings defy the logic of your background assumptions, which you should have translated as clearly as possible as working hypotheses at the top of your paper. When a negative finding shows up, assess to what extent it affects the general architecture of your model. For example, if one particular factor that you expected to play a critical role turns out to be statistically insignificant, then you have to \emph{revise} that expectation, not \emph{cancel} it.

When confronted to negative results, always take a step back at your model and ask whether it could be diagnosed as an issue in your specifications, i.e. your choice of data and methods. The quality of your data can be excellent but turn out to carry insufficient statistical power. Identically, linear regression, as explained in the previous sections, works as a hammer, and your research question might be hitting at something else than a nail, with mixed success.

\paragraph{Not-so-positive findings} Not-so-positive findings are statistically significant findings that end up being disappointing at the substantive level. The magnitude of the effects in your model, and its interpretation in natural units, should be your benchmarks to determine how substantively significant your findings are. Rather than worrying about explaining the total variance of your equation system, you should indeed be more concerned at that stage with statisticallly significant predictors with incomprehensible units of measurement or very weak regression coefficients.

In the case where a predictor is `under-performing' in your model, it might simply be that linear regression is not the most appropriate method for your data, which is a perfectly acceptable conclusion if you can explain why it was not possible to capture the relationship under scrutiny with a linear regression model. In the case where your dependent variable is susceptible to be the cause of the issue, you are advised to consider recoding it to a binary variable and switch to logistic regression.

%

\section{Formatting}

This section shows how to produce the tables mentioned in the paper template. The examples below use variables from the \qog{} dataset:

\begin{verbatim}
// example data
use bl_lu_25mf undp_gii wvs_abort wvs_theo unna_pop wdi_gdpc using data/qog2013, clear
// log-transformations
gen log_pop = ln(unna_pop)
la var log_pop "Log(Population)"
gen log_gdpc = ln(wdi_gdpc)
la var log_gdpc "Log(GDP per capita)"
\end{verbatim}

\paragraph{Summary statistics}
\index{Descriptive statistics!Table format}

The \cmd{stab} program, which is part of the course setup that you ran at p.~\pageref{sec:course-setup}, lets you export plain text tab-separated tables of summary statistics. These tables are convenient to open in spreadsheet editors like Microsoft Excel or OpenOffice Calc, and can be pasted into Google Documents.

The \cmd{stab} command is a wrapper for some of the export functions in the \cmd{estout} package. There are other ways to export summary statistics in more sophisticated ways with \cmd{tabout} and its supplementary command \opt{tabstatout}{tabout}.

\subsection{Correlation matrix}%
  \index{Correlation!Matrixes}

Table~\ref{tbl:estout-corr} shows a correlation matrix exported with the \cmd{estout} package, as shown at p.~\pageref{tbl:correlate_export}. 

% The numbering system saves space on paper, and columns are aligned on the decimal point to increase readability. Variable labels are preferable to less informative variable names. Last, remember to stick a complete caption. \emph{Note:} if you want to use cell colors to `warm up' the table, please feel free to do so, using a soft color theme.

\begin{fullwidth}
	\begin{table}
		\footnotesize
    %
    \input{T_estout_correlation}
		%
		\caption{Correlation output produced with \cmd{estout} and edited by adding variable numbers.}
		\label{tbl:estout-corr}
	\end{table}
\end{fullwidth}

\begin{verbatim}
// export correlation matrix with -estout-
keep wvs_abort bl_lu_25mf log_*
qui estpost correlate *, matrix listwise
esttab using correlates.csv, replace unstack not compress label nonum
\end{verbatim}

\index{Linear regression!Exporting results}
\paragraph{Regression output} Table~\ref{tbl:estout-reg} shows regression output exported with \cmd{estout} as shown at p.~\pageref{tbl:hibbs_yx1_estout}. Models are each in a separate column with a short title, and a lot of information has been dropped to reflect the most relevant aspects of the model. If you need more sophisticated output, Ben Jann's online documentation for \cmd{estout} contains useful examples.%
\footnote{\url{http://repec.org/bocode/e/estout/index.html}.}

% If you are focusing on standardized coefficients, you can produce an even more simplified output with the \opt{beta}{esttab} and \texttt{not} options.

\begin{fullwidth}
	\begin{table}
		\footnotesize
    %
    \input{T_estout_regression}
		%
		\caption{Regression output produced with \cmd{estout}.}
		\label{tbl:estout-reg}
	\end{table}
\end{fullwidth}

To produce this example, we first stored some example specifications of a linear regression model into short names \texttt{m1} to \texttt{m4}. To store a model, simply prefix its command with \texttt{eststo (name): qui}, which will quietly run the model and save it under \texttt{(name)}:

\begin{verbatim}
// example models
eststo m1: qui reg wvs_abort bl_lu_25mf log_* c.undp_gii#c.wvs_theo
eststo m2: qui reg wvs_abort bl_lu_25mf log_*
eststo m3: qui reg wvs_abort bl_lu_25mf log_* wvs_theo
eststo m4: qui reg wvs_abort bl_lu_25mf log_* undp_gii
\end{verbatim}

Note that the full model has an interaction term, and that the three other models simply show how the model behaves under slightly different specifications. The entire output is exported with a single command, \cmd{esstab}:

\begin{verbatim}
// export regression output with -estout-
esttab m1 m2 m3 m4 using models.csv, ///
  replace label b(1) se(1) sca(rmse) ///
	mti("Full model" "Controls" "Theocracy" "GII")
\end{verbatim}

The \texttt{b(1)} and \texttt{se(1)} options set the unstandardised coefficients and standard errors to one digit precision. The \texttt{sca(rmse)} option adds the RMSE to each model, for which names are set by the \texttt{mti} option.
