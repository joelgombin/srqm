%
% 3
%
\section{Advanced data skills}

The following operations are \emph{not} covered in the course because they require extensive checks that would take too much time to process. You are strongly advised \emph{not} to try any of the operations below, unless you can allocate several additional working hours to your project in the first weeks of class.

% 3.1
%
\subsection{Cross-dataset pooled questions}

Some polling items are close enough in question wording to be pooled in order to increase sample size and/or periodicity. Some attitudinal surveys, including some used in the course datasets, also use common rotating modules to provide cross-national comparability. Pooling can introduce a vast range of issues that require sensitivity checks to ensure that the pooling does not create idiosyncratic effects, thereby threatening the validity of the data.

When done right, pooling can also correct for many issues rather than creating new ones. Electoral forecasts, which do not aim at explaining the determinants of an election but rather to extract the predicted score from pre-electoral polls, can use pooling to produce very precise models. The most recent example with remarkable accuracy was Nate Silver's `538' electoral forecast for the \emph{New York Times}, which perfectly predicted the electoral college of the U.S. presidential election of November 2012. The methodology is not entirely clear, but it rests on a lot of pooled time series data.%
\footnote{\url{http://fivethirtyeight.blogs.nytimes.com/methodology/}} %
Drew Linzer's \emph{Votamatic} model has a similar methodology and more details.%
\footnote{\url{http://votamatic.org/how-it-works/}}

% 3.2
%
\subsection{Reshaping and collapsing}

Time series data can come in two formats, depending on whether there is one row of observation per year (`wide' format) or several (`long' format). The \cmd{reshape} command can convert from one to the other format.
    
The course datasets are cross-sectional and do not require reshaping prior to analysis.

% 3.3
%
\subsection{Fixed-format data}

Fixed-format data consists of variables that have been separated by a fixed number of spaces. This format is more complex to manipulate, as you have to determine the precise format of the data and run extensive code to name variables and apply labels, sometimes using a `dictionary' file.
    
Details on how to proceed with fixed-format data in Stata appear in the help page of the \cmd{infix} command, and short examples can be retrieved from online Stata tutorials.
    
For an example with the National Health Interview Survey of 2009%
\footnote{\url{http://www.cdc.gov/nchs/nhis/nhis_2009_data_release.htm}}, %
you can try downloading and uncompressing one of the \textsc{ascii} data files for that year%
\footnote{e.g. \url{ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHIS/2009/personsx.exe} (provided in self-extracting \textsc{exe} format)}, %
and then apply the sample Stata statements provided by the data teams to see what is involved in producing the final dataset.%
\footnote{e.g. \url{ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Program_Code/NHIS/2009/personsx.do}}
    
    If you try to process the example above, you will realize that fixed format data is difficult to manipulate. %
    Unless you have a \emph{lot} of time on your hands to convert and debug the files, do not engage into complex data wrangling with fixed format data.

% coda:

Last, datasets produced for \textsc{sas}, \textsc{spss} or other statistical software can be converted to Stata using the Stat/Transfer software.%
  \footnote{\url{http://www.stattransfer.com/}} %
StataCorp itself recommends the software,%
  \footnote{\url{http://www.stata.com/products/stat-transfer/}} %
and Sciences Po has made it available on its microlab workstations.

A free alternative consists in using the R statistical software with the \texttt{foreign} and \texttt{Hmisc} libraries to import the data in R and then export it to Stata.%
  \footnote{\url{http://www.statmethods.net/input/importingdata.html}} %
There is absolutely no guarantee, however, that the conversion process will be error-free.

In both cases, if you are taking it on yourself to convert the data rather than to trust the data team responsible for the original data files, then you are exposing yourself to conversion errors and will have to perform time-consuming verifications on your unofficial dataset.

For these reasons, your safest option is either to use official data files provided in the Stata proprietary dataset, or to rely on a machine-readable format like \textsc{csv} to be able to open the file from any software.
